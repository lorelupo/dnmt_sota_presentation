
@article{nikita_kitaev_reformer_nodate,
	title = {{REFORMER}: {THE} {EFFICIENT} {TRANSFORMER}},
	url = {https://arxiv.org/pdf/2001.04451.pdf},
	abstract = {Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transform- ers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L2 ) to O(L log L), where L is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training pro- cess instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.},
	author = {Nikita Kitaev, ≈Åukasz Kaiser and Anselm Levskaya},
	note = {00002 
This paper propose an efficient model (memory efficient and fast) on very long sequences ={\textgreater} since document level NMT is also a matter of handling longer sequences, i find it relevant for Lorenzo's PhD},
	keywords = {read}
}

@article{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2020-01-14},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {10171 
arXiv: 1409.0473},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/PD5B65U8/1409.html:text/html}
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2020-01-14},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {05728 
arXiv: 1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/RKCWKDXJ/1706.html:text/html}
}

@misc{phd_bert_2019,
	title = {{BERT}, {RoBERTa}, {DistilBERT}, {XLNet} ‚Äî which one to use?},
	url = {https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8},
	abstract = {Google‚Äôs BERT and recent transformer-based methods have taken the NLP landscape by a storm, outperforming the state-of-the-art on several‚Ä¶},
	language = {en},
	urldate = {2020-01-20},
	journal = {Medium},
	author = {Ph.D, Suleiman Khan},
	month = oct,
	year = {2019},
	note = {00000},
	file = {Snapshot:/home/lupol/Zotero/storage/4CTGKY4C/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8.html:text/html}
}

@article{miculicich_document-level_2018,
	title = {Document-{Level} {Neural} {Machine} {Translation} with {Hierarchical} {Attention} {Networks}},
	url = {http://arxiv.org/abs/1809.01576},
	abstract = {Neural Machine Translation (NMT) can be improved by including document-level contextual information. For this purpose, we propose a hierarchical attention model to capture the context in a structured and dynamic manner. The model is integrated in the original NMT architecture as another level of abstraction, conditioning on the NMT model's own previous hidden states. Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.},
	urldate = {2020-01-20},
	journal = {arXiv:1809.01576 [cs]},
	author = {Miculicich, Lesly and Ram, Dhananjay and Pappas, Nikolaos and Henderson, James},
	month = oct,
	year = {2018},
	note = {00024 
arXiv: 1809.01576},
	keywords = {Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/JLC9BK82/1809.html:text/html}
}

@article{maruf_selective_2019,
	title = {Selective {Attention} for {Context}-aware {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1903.08788},
	abstract = {Despite the progress made in sentence-level NMT, current systems still fall short at achieving fluent, good quality translation for a full document. Recent works in context-aware NMT consider only a few previous sentences as context and may not scale to entire documents. To this end, we propose a novel and scalable top-down approach to hierarchical attention for context-aware NMT which uses sparse attention to selectively focus on relevant sentences in the document context and then attends to key words in those sentences. We also propose single-level attention approaches based on sentence or word-level information in the context. The document-level context representation, produced from these attention modules, is integrated into the encoder or decoder of the Transformer model depending on whether we use monolingual or bilingual context. Our experiments and evaluation on English-German datasets in different document MT settings show that our selective attention approach not only significantly outperforms context-agnostic baselines but also surpasses context-aware baselines in most cases.},
	urldate = {2020-01-20},
	journal = {arXiv:1903.08788 [cs]},
	author = {Maruf, Sameen and Martins, Andr√© F. T. and Haffari, Gholamreza},
	month = may,
	year = {2019},
	note = {00012},
	keywords = {Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/JG76XJZW/1903.html:text/html}
}

@article{yang_context-aware_2019,
	title = {Context-{Aware} {Self}-{Attention} {Networks}},
	url = {http://arxiv.org/abs/1902.05766},
	abstract = {Self-attention model have shown its flexibility in parallel computation and the effectiveness on modeling both long- and short-term dependencies. However, it calculates the dependencies between representations without considering the contextual information, which have proven useful for modeling dependencies among neural representations in various natural language tasks. In this work, we focus on improving self-attention networks through capturing the richness of context. To maintain the simplicity and flexibility of the self-attention networks, we propose to contextualize the transformations of the query and key layers, which are used to calculates the relevance between elements. Specifically, we leverage the internal representations that embed both global and deep contexts, thus avoid relying on external resources. Experimental results on WMT14 English-German and WMT17 Chinese-English translation tasks demonstrate the effectiveness and universality of the proposed methods. Furthermore, we conducted extensive analyses to quantity how the context vectors participate in the self-attention model.},
	urldate = {2020-01-20},
	journal = {arXiv:1902.05766 [cs]},
	author = {Yang, Baosong and Li, Jian and Wong, Derek and Chao, Lidia S. and Wang, Xing and Tu, Zhaopeng},
	month = feb,
	year = {2019},
	note = {00000 
arXiv: 1902.05766},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/C6UE48MG/1902.html:text/html}
}

@misc{noauthor_cs_nodate,
	title = {{CS} 11-747: {Neural} {Networks} for {NLP}},
	url = {http://phontron.com/class/nn4nlp2019/schedule/attention.html},
	urldate = {2020-01-23},
	note = {00000},
	keywords = {read},
	file = {CS 11-747\: Neural Networks for NLP:/home/lupol/Zotero/storage/T67RU8GK/attention.html:text/html}
}

@article{srivastava_highway_2015,
	title = {Highway {Networks}},
	url = {http://arxiv.org/abs/1505.00387},
	abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
	urldate = {2020-01-23},
	journal = {arXiv:1505.00387 [cs]},
	author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J√ºrgen},
	month = nov,
	year = {2015},
	note = {00000 
arXiv: 1505.00387},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, I.2.6, 68T01, G.1.6},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/T2RCDF3L/1505.html:text/html}
}

@inproceedings{kovaleva_revealing_2019,
	title = {Revealing the {Dark} {Secrets} of {BERT}},
	url = {https://www.aclweb.org/anthology/D19-1445},
	doi = {10.18653/v1/D19-1445},
	abstract = {Olga Kovaleva, Alexey Romanov, Anna Rogers, Anna Rumshisky. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.},
	language = {en-us},
	urldate = {2020-01-27},
	author = {Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},
	month = nov,
	year = {2019},
	note = {00004},
	pages = {4365--4374},
	file = {Snapshot:/home/lupol/Zotero/storage/3ME5P3PF/D19-1445.html:text/html}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2020-01-27},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {38089 
arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/HCNUIPW2/1512.html:text/html}
}

@misc{shorten_introduction_2019,
	title = {Introduction to {ResNets}},
	url = {https://towardsdatascience.com/introduction-to-resnets-c0a830a288a4},
	abstract = {This Article is Based on Deep Residual Learning for Image Recognition from He et al. [2] (Microsoft Research)‚Ä¶},
	language = {en},
	urldate = {2020-01-27},
	journal = {Medium},
	author = {Shorten, Connor},
	month = may,
	year = {2019},
	note = {00000},
	file = {Snapshot:/home/lupol/Zotero/storage/JI5C639M/introduction-to-resnets-c0a830a288a4.html:text/html}
}

@article{gehring_convolutional_2017,
	title = {Convolutional {Sequence} to {Sequence} {Learning}},
	url = {http://arxiv.org/abs/1705.03122},
	abstract = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.},
	urldate = {2020-02-04},
	journal = {arXiv:1705.03122 [cs]},
	author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.},
	month = jul,
	year = {2017},
	note = {01149 
arXiv: 1705.03122},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/EDRM5IER/1705.html:text/html}
}

@misc{noauthor_annotated_2020,
	title = {The {Annotated} {GPT}-2},
	url = {https://amaarora.github.io/2020/02/18/annotatedGPT2.html},
	abstract = {Introduction Prerequisites Language Models are Unsupervised Multitask Learners Abstract Model Architecture (GPT-2) Model Specifications (GPT) Imports Transformer Decoder inside GPT-2 CONV1D Layer Explained FEEDFORWARD Layer Explained ATTENTION Layer Explained Scaled Dot-Product Attention Multi-Head Attention GPT-2 Model Architecture in Code Transformer Decoder Block Explained The GPT-2 Architecture Explained Language Modeling or Classification Sample text generation using Hugging Face Pretrained Weights Extras Credits Feedback Introduction Welcome to ‚ÄúThe Annotated GPT-2‚Äù. One of the most brilliant and well-explained articles I have ever read is The Annotated Transformer. It introduced Attention like no other post ever written. The simple idea was to present an ‚Äúannotated‚Äù version of the paper Attention is all you need along with code. Something I have come to realize with my little experience in Machine Learning, when you write things in code, the implementation and the secrets become clearer. It is not magic anymore. There is nothing magic about magic. The magician merely understands something simple which doesn‚Äôt appear to be simple or natural to the untrained audience. Once you learn how to hold a card while making your hand look empty, you only need practice before you, too, can ‚Äúdo magic.‚Äù ‚Äì Jeffrey Friedl in the book Mastering Regular Expressions The GPT-2 might seem like magic at first with all it‚Äôs glitter and beauty too, but hopefully I would have uncovered that magic for you and revealed all the tricks by the time you finish reading this post. That is my goal. To make it as simple as possible for the keen to understand how the GPT-2 model works underneath. Note: Pretty much the entirety of the code has been copied, inspired and referenced from Hugging Face‚Äôs implementation of the GPT-2, keeping merely the essentials for simplicity. If you want to train the GPT-2 model on parallel GPUs, save checkpoints while fine-tuning, run inference tasks on multiple CPUs and much more, I would recommend using the Hugging Face API. A simple tutorial on how to do so was recently released by Hugging Face and can be found here. In this post, I am not trying to reinvent the wheel, but merely bringing together a list of prexisting excellent resources to make it easier for the reader to grasp GPT-2. I leave it up to the reader to further build upon these foundations in any area they choose. You can‚Äôt build a great building on a weak foundation. You must have a solid foundation if you‚Äôre going to have a strong superstructure. ‚Äì Gordon B. Hinckley Prerequisites This post assumes that the reader has a solid understanding of Attention and Transformers. The GPT-2 utilizes a 12-layer Decoder Only Transformer architecture. If you want a refresher or understand Attention and Transformers, here is an excellent list of resources to aid your understanding regarding: The illustrated Transformer by Jay Alammar The Annotated Transformer by Harvard NLP Introduction to the Transformer by Rachel Thomas and Jeremy Howard If you‚Äôre just beginning your journey into NLP or you‚Äôre an expert, I would definitely recommend the fast.ai NLP course taught by Rachel Thomas and Jeremy Howard. The course starts with the basics including Sentiment Classification using Naive Bayes and Logistic Regression, moves on to RNNs and also talks about Transfer Learning, ULMFiT, Seq2Seq translation and Transformers amongst other things. It is an excellent resource put together by the fast.ai team free of cost. Another amazing resource on GPT-2 itself, is The Illustrated GPT-2 by Jay Alammar. This post starts with a basic introduction to Language Models and explains the GPT-2 model step-by-step in a very easy to understand manner. I would highly recommend the reader to give this post a read. The Annotated Transformer by Harvard NLP implements the complete Transformer architecture using PyTorch and is great way to understand Attention in depth. Let‚Äôs then build upon these excellent existing resources and implement GPT-2 in code. Language Models are Unsupervised Multitask Learners Abstract Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations. A Zero-shot setting is one where you do not finetune the language model and directly run inference on the target dataset. For example, pretrain a LM on WebText and directly try and predict the next words of Amazon Movie reviews dataset. Model Architecture (GPT-2) We use a Transformer (Vaswani et al., 2017) based architecture for our LMs. The model largely follows the details of the OpenAI GPT model (Radford et al., 2018) with a few modifications. Layer normalization (Ba et al., 2016) was moved to the input of each sub-block, similar to a pre-activation residual network (He et al., 2016) and an additional layer normalization was added after the final self-attention block. We scale the weights of residual layers at initialization by a factor of 1/‚àöN where N is the number of residual layers. The vocabulary is expanded to 50,257 words. We also increase the context size from 512 to 1024 tokens and a larger batchsize of 512 is used. This is the entirety of model explanation inside the GPT-2 research paper. This warrants a need for us to look at the architecture inside the GPT model. Model Specifications (GPT) Our model largely follows the original transformer work. We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states. We used the Adam optimization scheme with a max learning rate of 2.5e-4. The learning rate was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule. We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens. Since layernorm is used extensively throughout the model, a simple weight initialization of N(0, 0.02) was sufficient. We used a bytepair encoding (BPE) vocabulary with 40,000 merges and residual, embedding, and attention dropouts with a rate of 0.1 for regularization. We also employed a modified version of L2 regularization proposed in, with w = 0.01 on all non bias or gain weights. For the activation function, we used the Gaussian Error Linear Unit (GELU). As can be seen from the GPT Architecture, to implement it, we will first need to implement Masked Self Attention and Feed Forward layer. Imports import torch import copy import torch.nn as nn import torch.nn.functional as F from torch.nn.modules import ModuleList from torch.nn.modules.normalization import LayerNorm import numpy as np import os from tqdm import tqdm\_notebook, trange import logging logging.basicConfig(level = logging.INFO) logger = logging.getLogger() Transformer Decoder inside GPT-2 To re-use the terminology used to describe the Transformer, the attention is a function of a query (Q) and set of key (K) and value (V) pairs. To handle longer sequences, we modify the multi-head self-attention of the Transformer to reduce memory usage by limiting the dot products between Q and K in: class Conv1D(nn.Module): def \_\_init\_\_(self, nx, nf): super().\_\_init\_\_() self.nf = nf w = torch.empty(nx, nf) nn.init.normal\_(w, std=0.02) self.weight = nn.Parameter(w) self.bias = nn.Parameter(torch.zeros(nf)) def forward(self, x): size\_out = x.size()[:-1] + (self.nf,) x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight) x = x.view(*size\_out) return x CONV1D Layer Explained The CONV1D layer can be thought of as a LINEAR layer itself. Essentially, it is casting an initial tensor x (having the final dimension of x.size(-1)) being passed to it to have a final dimension of size self.nf. Here‚Äôs an example output of the same: d\_model = 768 conv1d = Conv1D(d\_model, d\_model*3) x = torch.rand(1,4,d\_model) \#represents a sequence of batch\_size=1, seq\_len=4 and embedding\_sz=768, something like "Hello how are you" x = conv1d(x) x.shape {\textgreater}{\textgreater} torch.Size([1, 4, 2304]) As can be seen in the example above, the final dimension of tensor returned by CONV1D is 3 times the initial size. We do this to be able to cast the input to query, key and value matrices. It is possible then to retrieve the query, key and value matrices like so: query, key, value = x.split(d\_model, dim=-1) query.shape, key.shape, value.shape {\textgreater}{\textgreater} (torch.Size([1, 4, 768]), torch.Size([1, 4, 768]), torch.Size([1, 4, 768])) Another way to cast the input to Q, K and V matrices would have to been to have separate Wq, Wk and Wv matrices. I have explained this under the EXTRA section of this post at the bottom. I find this other approach more intuitive and relatable, but we use the CONV1D layer in this post, because we reuse the CONV1D pretrained weights from Hugging Face. FEEDFORWARD Layer Explained class FeedForward(nn.Module): def \_\_init\_\_(self, dropout, d\_model=768, nx=768*4): super().\_\_init\_\_() self.c\_fc = Conv1D(d\_model, nx) self.c\_proj = Conv1D(nx, d\_model) self.act = F.gelu self.dropout = nn.Dropout(dropout) def forward(self, x): return self.dropout(self.c\_proj(self.act(self.c\_fc(x)))) Something, that‚Äôs just so well explained in Jay Alammar‚Äôs post - also referenced above, is how the inputs are passed through ATTENTION layer first and then on to FEEDFORWARD layer. The Feedforward network, is a normal nueral network that accepts the outputs from the ATTENTION layer (768), casts them to nx (768*4) dimension, adds an activation function self.act (GELU), casts them back to d\_model (768) and adds dropout (0.1). This is also mentioned in the GPT research paper referenced below. For the position-wise feed-forward networks, we used 3072 dimensional inner states ATTENTION Layer Explained The below extract is from the paper Attention is all you need. Scaled Dot-Product Attention We call our particular attention ‚ÄúScaled Dot-Product Attention‚Äù. The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by ‚àödk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as: The two most commonly used attention functions are additive attention, and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1/‚àödk. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by 1/‚àödk. To implement the The Attention layer in code, we first utilize the CONV1D layer and get the q, k and v matrices as explained before. Once we have the q, k and v matrices, we can perform attention using the function \_attn. This function replicates the formula mentioned above inside Attention Dot Product. class Attention(nn.Module): def \_\_init\_\_(self, d\_model=768, n\_head=12, n\_ctx=1024, d\_head=64, bias=True, scale=False): super().\_\_init\_\_() self.n\_head = n\_head self.d\_model = d\_model self.c\_attn = Conv1D(d\_model, d\_model*3) self.scale = scale self.softmax = nn.Softmax(dim=-1) self.register\_buffer("bias", torch.tril(torch.ones(n\_ctx, n\_ctx)).view(1, 1, n\_ctx, n\_ctx)) self.dropout = nn.Dropout(0.1) self.c\_proj = Conv1D(d\_model, d\_model) def split\_heads(self, x): "return shape [`batch`, `head`, `sequence`, `features`]" new\_shape = x.size()[:-1] + (self.n\_head, x.size(-1)//self.n\_head) x = x.view(*new\_shape) return x.permute(0, 2, 1, 3) def \_attn(self, q, k, v, attn\_mask=None): scores = torch.matmul(q, k.transpose(-2, -1)) if self.scale: scores = scores/math.sqrt(v.size(-1)) nd, ns = scores.size(-2), scores.size(-1) if attn\_mask is not None: scores = scores + attn\_mask scores = self.softmax(scores) scores = self.dropout(scores) outputs = torch.matmul(scores, v) return outputs def merge\_heads(self, x): x = x.permute(0, 2, 1, 3).contiguous() new\_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),) return x.view(*new\_shape) def forward(self, x): x = self.c\_attn(x) \#new `x` shape - `[1,3,2304]` q, k, v = x.split(self.d\_model, dim=2) q, k, v = self.split\_heads(q), self.split\_heads(k), self.split\_heads(v) out = self.\_attn(q, k, v) out = self.merge\_heads(out) out = self.c\_proj(out) return out Another way to implement Attention is explained in the Extras section at the bottom of this blog. I find it to be more intuitive and easy to compare with the research paper. It utilizes Linear layers instead of CONV1D to cast inputs to Q, K and V matrices. The reason why we haven‚Äôt used it is because we use the pretrained weights for CONV1D layer from Hugging Face. Multi-Head Attention The below extract is from the paper Attention is all you need. Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure below. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. Not to be confused by this, in essence all that‚Äôs being done is to add another dimension to the Q, K and V matrices. That is, if the matrices were before of size [1, 4, 768] which represents [bs, seq\_len, d\_model], these matrices are projected to dimension [1, 12, 4, 64] which represents [bs, n\_head, seq\_len, d\_model//n\_head]. GPT-2 utizes 12 parallel heads. We split the Q, K, V matrices inside split\_heads function. Finally, once we get an output from applying parallel attentions we concatenate it inside merge\_heads back to matrices of dimension [bs, seq\_len, d\_model]. GPT-2 Model Architecture in Code So far, we have implemented Multi Head Attention and FeedForward layers. The two layers form the building blocks of the Transformer Decoder block, shown in the picture above. The GPT-2 consists of 12 of these Transformer Blocks. This has been shown in Jay Alammar‚Äôs post like so: Transformer Decoder Block Explained class TransformerBlock(nn.Module): def \_\_init\_\_(self, d\_model=768, n\_head=12, dropout=0.1): super(TransformerBlock, self).\_\_init\_\_() self.attn = Attention(d\_model=768, n\_head=12, d\_head=64, n\_ctx=1024, bias=True, scale=False) self.feedforward = FeedForward(dropout=0.1, d\_model=768, nx=768*4) self.ln\_1 = LayerNorm(d\_model) self.ln\_2 = LayerNorm(d\_model) def forward(self, x): x = x + self.attn(self.ln\_1(x)) x = x + self.feedforward(self.ln\_2(x)) return x The Transformer Block consists of Attention and FeedForward Layers. As referenced from the GPT-2 Architecture Model Specification, Layer normalization (Ba et al., 2016) was moved to the input of each sub-block Here are the sub-blocks are Attention and FeedForward. Thus, inside a Transformer Decoder Block, essentially we first pass the inputs to a LayerNorm followed by the first sub-block Attention. Next, we pass the outputs of this sub-block to LayerNorm again and finally to FeedForward layer. The GPT-2 Architecture Explained As referenced from the GPT paper, We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). Thus, the complete GPT-2 architecture is the TransformerBlock copied over 12 times. def \_get\_clones(module, n): return ModuleList([copy.deepcopy(module) for i in range(n)]) class GPT2(nn.Module): def \_\_init\_\_(self, nlayers=12, n\_ctx=1024, d\_model=768, vcb\_sz=50257): super(GPT2, self).\_\_init\_\_() self.nlayers = nlayers block = TransformerBlock(d\_model=768, n\_head=12, dropout=0.1) self.h = \_get\_clones(block, 12) self.wte = nn.Embedding(vcb\_sz, d\_model) self.wpe = nn.Embedding(n\_ctx, d\_model) self.drop = nn.Dropout(0.1) self.ln\_f = LayerNorm(d\_model) self.out = nn.Linear(d\_model, vcb\_sz, bias=False) self.loss\_fn = nn.CrossEntropyLoss() self.init\_weights() def init\_weights(self): self.out.weight = self.wte.weight self.apply(self.\_init\_weights) def \_init\_weights(self, module): if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)): module.weight.data.normal\_(mean=0.0, std=0.02) if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None: module.bias.data.zero\_() elif isinstance(module, nn.LayerNorm): module.bias.data.zero\_() module.weight.data.fill\_(1.0) def forward(self, src, labels=None, pos\_ids=None): if pos\_ids is None: pos\_ids = torch.arange(0, src.size(-1)).unsqueeze(0) inp = self.drop((self.wte(src)+self.wpe(pos\_ids))) for i in range(self.nlayers): inp = self.h[i](inp) inp = self.ln\_f(inp) logits = self.out(inp) outputs = (logits,) + (inp,) if labels is not None: shift\_logits = logits[..., :-1, :].contiguous() shift\_labels = labels[..., 1:].contiguous() loss = self.loss\_fn(shift\_logits.view(-1, shift\_logits.size(-1)), shift\_labels.view(-1)) outputs = (loss,) + outputs return outputs return logits Something I have not mentioned yet is Positional Encoding and Token Embeddings. Since, we cannot pass words such as ‚Äúhey‚Äù or ‚Äúhello‚Äù directly to the model, we first Tokenize our inputs. Next, we use Embeddings to represent the tokens as numbers. This post by Jay Alammar again explains Embeddings very well. Also, since unlike the RNNs where the input words are passed sequentially, Transformers take input matrices in parallel thus losing the sense of position for the words being input. To make up for the loss, before handling the Token Embeddings to the model, we add Positional Encoding - a signal that indicates the order of the words in the sequence. Since, as mentioned before, the context size of GPT-2 is 1024, the positional encodings are of dimensions [1024, 768]. Thus, the inputs to the GPT-2 architecture is the sum of Token Embeddings and Positional Encodings passed through a Dropout, to add regularization. Once, we have the input matrix, we pass this through each of the 12 Layers of the GPT-2 architecure, where each layer is a Transformer Decoder Block that consists of two sublayers - Attention and FeedForward Network. Language Modeling or Classification When using GPT-2 as a language model, we pass the inputs to a final LayerNorm and through a Linear layer with a final dimension of size [768, vocab\_sz] (50257) and get an output of size [1, 4, 50257]. This output represents the next word logits and we can very easily now pass this through a Softmax layer and take argmax to get the positional of the word inside the vocabulary with the highest probability. For classification task, we can pass the outputs received from the GPT-2 architecture through a Linear layer with a dimension of size [768, n] to get probabilities for each category (where n represents number of categories), pass it through a softmax, get the highest predicted category and use CrossEntropyLoss to train the architecture to do classification. And that‚Äôs really all the magic behind GPT-2. It‚Äôs a Decoder only Transformer Based architecture that takes inputs parallely with Positional Encodings unlike RNNs, passes them through each of it‚Äôs 12 Transformer Decoder layers (which consist of Multi head Attention and FeedForward Network) to return the final output. Let‚Äôs see this model in action in a language model task. Sample text generation using Hugging Face Pretrained Weights First, let‚Äôs initialize the model with the Pretrained Weights already provided by Hugging Face. model = GPT2() \# load pretrained\_weights from hugging face \# download file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch\_model.bin to `.` model\_dict = model.state\_dict() \#currently with random initialization state\_dict = torch.load("./gpt2-pytorch\_model.bin") \#pretrained weights old\_keys = [] new\_keys = [] for key in state\_dict.keys(): if "mlp" in key: \#The hugging face state dict references the feedforward network as mlp, need to replace to `feedforward` be able to reuse these weights new\_key = key.replace("mlp", "feedforward") new\_keys.append(new\_key) old\_keys.append(key) for old\_key, new\_key in zip(old\_keys, new\_keys): state\_dict[new\_key]=state\_dict.pop(old\_key) pretrained\_dict = \{k: v for k, v in state\_dict.items() if k in model\_dict\} model\_dict.update(pretrained\_dict) model.load\_state\_dict(model\_dict) model.eval() \#model in inference mode as it's now initialized with pretrained weights Let‚Äôs now generate text. We will utilize Hugging Face‚Äôs pretrained Tokenizer to convert words to input embeddings. from transformers import GPT2Tokenizer tokenizer = GPT2Tokenizer.from\_pretrained("gpt2") context = torch.tensor([tokenizer.encode("The planet earth")]) def generate(context, ntok=20): for \_ in range(ntok): out = model(context) logits = out[:, -1, :] indices\_to\_remove = logits {\textless} torch.topk(logits, 10)[0][..., -1, None] logits[indices\_to\_remove] = np.NINF next\_tok = torch.multinomial(F.softmax(logits, dim=-1), num\_samples=1).squeeze(1) context = torch.cat([context, next\_tok.unsqueeze(-1)], dim=-1) return context out = generate(context, ntok=20) tokenizer.decode(out[0]) {\textgreater}{\textgreater} 'The planet earth is the source of all of all the light," says the study that the government will' Extras Another way to implement Attention as shown in the NLP Course by fast.ai referenced from here, that I find to be more intuitive is as below: class Attention\_FASTAI(nn.Module): def \_\_init\_\_(self, d\_model=768, n\_head=12, d\_head=64, n\_ctx=1024, bias=True, scale=False): super().\_\_init\_\_() self.n\_head = n\_head self.d\_head = d\_head self.softmax = nn.Softmax(dim=-1) self.scale = scale self.atn\_drop = nn.Dropout(0.1) self.wq, self.wk, self.wv = [nn.Linear(d\_model, n\_head*d\_head, bias=bias) for o in range(3)] def split\_heads(self, x, layer, bs): x = layer(x) return x.view(bs, x.size(1), self.n\_head, self.d\_head).permute(0,2,1,3) def \_attn(self, q, k, v, attn\_mask=None): scores = torch.matmul(q, k.transpose(-2, -1)) if self.scale: scores = scores/math.sqrt(v.size(-1)) if attn\_mask is not None: scores = scores.float().masked\_fill(attn\_mask, -float('inf')).type\_as(scores) attn\_prob = self.atn\_drop(self.softmax(scores)) attn\_vec = attn\_prob @ v return attn\_vec def merge\_heads(self, x, bs, seq\_len): x = x.permute(0, 2, 1, 3).contiguous() return x.view(bs, seq\_len, -1) def forward(self, q, k, v, mask=None): bs, seq\_len = q.size(0), q.size(1) wq, wk, wv = map(lambda o:self.split\_heads(*o, bs), zip((q,k,v), (self.wq, self.wk, self.wv))) attn\_vec = self.\_attn(wq, wk, wv) attn\_vec = self.merge\_heads(attn\_vec, bs, seq\_len) return attn\_vec The key difference between the implementation above and the one we have used is that this implementation does not use CONV1D. Instead, we first pass the input x to self.wq, self.wk and self.wv Linear Layers to get wq, wk and wv matrices and then perform attention as before. Credits I just want to take the time to thank Rachel Thomas and Jeremy Howard for a great NLP course and the fast.ai course in general, which has helped me bolster my understanding of RNNs, GRUs, AWD-LSTM and Transformers. Also, a special thanks to Hugging Face for creating an open source NLP library and providing a number of Pretrained Models to use. As mentioned the code in this blog post comes directly from the Hugging Face library. And, Jay Alammar for the excellent work that he has been doing to Visualise machine learning concepts. The Illustrated GPT-2 is one of the most comprehensive blog posts on GPT-2. Finally, to Harvard NLP, for The Annotated Transformer, a beautiful and easy to follow implementation of Transformers in PyTorch. Feedback Comments or feedback? Please tweet me at @amaarora This is a really wonderful resource, and draws together many very nice pieces of work. https://t.co/CM16ByNrbt‚Äî Jeremy Howard (@jeremyphoward) February 19, 2020 Great work pairing GPT2 concepts with the key excerpts from the code. https://t.co/IkFlAf3Ua8‚Äî Jay Alammar ÿ¨ŸáÿßÿØ ÿßŸÑÿπŸÖÿßÿ± (@jalammar) February 20, 2020 "The Annotated GPT-2" blogpost seems to start out a a simple question of asking why use conv-1d vs linear. An awesome read!! https://t.co/GCju0z3Wri \#nlproc \#nlposs \#distiller https://t.co/cvSdEah8gT‚Äî Liling Tan (@alvations) February 20, 2020 A must-read blog about GPT-2. https://t.co/EuDil5Dm07‚Äî Xinhao Li (@XinhaoLi1) February 20, 2020 One of the best NLP Blogposts I've read: A definitive and complete writeup. üçµThis is a blog, I wish I had when I was tinkering with the GPT-2. Must read for everyone: https://t.co/yLRFywYgm6‚Äî Sanyam Bhutani (@bhutanisanyam1) February 20, 2020 Neat! https://t.co/eLt3o180Qr‚Äî Ant√¥nio Horta Ribeiro (@ahortaribeiro) February 20, 2020 Fantastic work!! Looking forward to learning what is it behind the scenes of this language model! https://t.co/Ml1DY22NxQ‚Äî Data Enigma (@EnigmaData) February 19, 2020 The Annotated GPT-2 - Understand how the GPT-2 model works underneath with explanations and source codeBlogpost https://t.co/anLqVhZQPN@amaaroraùê¨ùê©ùê´ùêûùêöùêù ùê≠ùê°ùêû ùê∞ùê®ùê´ùêù ùê®ùêü \#ùêçùêãùêè üíú\#datascience \#pytorch \#deeplearning \#machinelearning pic.twitter.com/n5QIQBAIfH‚Äî Philip Vollet (Ôæâ‚óï„ÉÆ‚óï)Ôæâ*:„ÉªÔæü‚úß (@philipvollet) February 20, 2020 This is fantastic @amaarora, thanks üëç‚Äî Manpreet Singh (@ms\_ghotratweet) February 20, 2020 This is brilliant stuff!!!‚Äî Arron Hovingham (@AnalystiaArron) February 20, 2020},
	language = {en},
	urldate = {2020-02-25},
	journal = {Making commits each day, towards a better future},
	month = feb,
	year = {2020},
	note = {00000},
	file = {Snapshot:/home/lupol/Zotero/storage/E7KPNU45/annotatedGPT2.html:text/html}
}

@misc{dirafzoon_illustrating_2020,
	title = {üí°{Illustrating} the {Reformer}},
	url = {https://towardsdatascience.com/illustrating-the-reformer-393575ac6ba0},
	abstract = {üöä Ô∏è The efficient Transformer},
	language = {en},
	urldate = {2020-02-25},
	journal = {Medium},
	author = {Dirafzoon, Alireza},
	month = feb,
	year = {2020},
	note = {00000},
	file = {Snapshot:/home/lupol/Zotero/storage/224SE43S/illustrating-the-reformer-393575ac6ba0.html:text/html}
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2020-02-25},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {03826 
arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/6VZS6E56/1810.html:text/html}
}

@article{chen_best_2018,
	title = {The {Best} of {Both} {Worlds}: {Combining} {Recent} {Advances} in {Neural} {Machine} {Translation}},
	shorttitle = {The {Best} of {Both} {Worlds}},
	url = {http://arxiv.org/abs/1804.09849},
	abstract = {The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT'14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.},
	urldate = {2020-02-28},
	journal = {arXiv:1804.09849 [cs]},
	author = {Chen, Mia Xu and Firat, Orhan and Bapna, Ankur and Johnson, Melvin and Macherey, Wolfgang and Foster, George and Jones, Llion and Parmar, Niki and Schuster, Mike and Chen, Zhifeng and Wu, Yonghui and Hughes, Macduff},
	month = apr,
	year = {2018},
	note = {00130 
arXiv: 1804.09849},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/EGN6HB5K/1804.html:text/html}
}

@inproceedings{sukhbaatar_adaptive_2019,
	address = {Florence, Italy},
	title = {Adaptive {Attention} {Span} in {Transformers}},
	url = {https://www.aclweb.org/anthology/P19-1032},
	doi = {10.18653/v1/P19-1032},
	abstract = {We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.},
	urldate = {2020-03-04},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
	month = jul,
	year = {2019},
	note = {00012},
	pages = {331--335}
}

@inproceedings{shaw_self-attention_2018,
	address = {New Orleans, Louisiana},
	title = {Self-{Attention} with {Relative} {Position} {Representations}},
	url = {https://www.aclweb.org/anthology/N18-2074},
	doi = {10.18653/v1/N18-2074},
	abstract = {Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.},
	urldate = {2020-03-13},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 2 ({Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
	month = jun,
	year = {2018},
	note = {00168},
	keywords = {read},
	pages = {464--468}
}

@article{wu_pay_2019,
	title = {Pay {Less} {Attention} with {Lightweight} and {Dynamic} {Convolutions}},
	url = {http://arxiv.org/abs/1901.10430},
	abstract = {Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.},
	urldate = {2020-03-13},
	journal = {arXiv:1901.10430 [cs]},
	author = {Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann N. and Auli, Michael},
	month = feb,
	year = {2019},
	note = {00073 
arXiv: 1901.10430},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/XNE9WZFB/1901.html:text/html}
}

@inproceedings{dai_transformer-xl_2019,
	address = {Florence, Italy},
	title = {Transformer-{XL}: {Attentive} {Language} {Models} beyond a {Fixed}-{Length} {Context}},
	shorttitle = {Transformer-{XL}},
	url = {https://www.aclweb.org/anthology/P19-1285},
	doi = {10.18653/v1/P19-1285},
	abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
	urldate = {2020-03-13},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc and Salakhutdinov, Ruslan},
	month = jul,
	year = {2019},
	note = {00238},
	keywords = {read},
	pages = {2978--2988}
}

@article{rae_compressive_2019,
	title = {Compressive {Transformers} for {Long}-{Range} {Sequence} {Modelling}},
	url = {http://arxiv.org/abs/1911.05507},
	abstract = {We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.},
	urldate = {2020-03-17},
	journal = {arXiv:1911.05507 [cs, stat]},
	author = {Rae, Jack W. and Potapenko, Anna and Jayakumar, Siddhant M. and Lillicrap, Timothy P.},
	month = nov,
	year = {2019},
	note = {00000 
arXiv: 1911.05507},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/N9ELUM5R/1911.html:text/html}
}