
@inproceedings{melamed_precision_2003,
	title = {Precision and {Recall} of {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/N03-2021},
	urldate = {2020-01-14},
	booktitle = {Companion {Volume} of the {Proceedings} of {HLT}-{NAACL} 2003 - {Short} {Papers}},
	author = {Melamed, I. Dan and Green, Ryan and Turian, Joseph P.},
	year = {2003},
	note = {00000},
	keywords = {read},
	pages = {61--63}
}

@inproceedings{banerjee_meteor_2005,
	address = {Ann Arbor, Michigan},
	title = {{METEOR}: {An} {Automatic} {Metric} for {MT} {Evaluation} with {Improved} {Correlation} with {Human} {Judgments}},
	shorttitle = {{METEOR}},
	url = {https://www.aclweb.org/anthology/W05-0909},
	urldate = {2020-01-16},
	booktitle = {Proceedings of the {ACL} {Workshop} on {Intrinsic} and {Extrinsic} {Evaluation} {Measures} for {Machine} {Translation} and/or {Summarization}},
	publisher = {Association for Computational Linguistics},
	author = {Banerjee, Satanjeev and Lavie, Alon},
	month = jun,
	year = {2005},
	note = {00000},
	pages = {65--72}
}

@inproceedings{koehn_statistical_2004,
	address = {Barcelona, Spain},
	title = {Statistical {Significance} {Tests} for {Machine} {Translation} {Evaluation}},
	url = {https://www.aclweb.org/anthology/W04-3250},
	urldate = {2020-01-16},
	booktitle = {Proceedings of the 2004 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Koehn, Philipp},
	month = jul,
	year = {2004},
	note = {00000},
	pages = {388--395}
}

@inproceedings{miculicich_werlen_validation_2017,
	address = {Copenhagen, Denmark},
	title = {Validation of an {Automatic} {Metric} for the {Accuracy} of {Pronoun} {Translation} ({APT})},
	url = {https://www.aclweb.org/anthology/W17-4802},
	doi = {10.18653/v1/W17-4802},
	abstract = {In this paper, we define and assess a reference-based metric to evaluate the accuracy of pronoun translation (APT). The metric automatically aligns a candidate and a reference translation using GIZA++ augmented with specific heuristics, and then counts the number of identical or different pronouns, with provision for legitimate variations and omitted pronouns. All counts are then combined into one score. The metric is applied to the results of seven systems (including the baseline) that participated in the DiscoMT 2015 shared task on pronoun translation from English to French. The APT metric reaches around 0.993-0.999 Pearson correlation with human judges (depending on the parameters of APT), while other automatic metrics such as BLEU, METEOR, or those specific to pronouns used at DiscoMT 2015 reach only 0.972-0.986 Pearson correlation.},
	urldate = {2020-01-16},
	booktitle = {Proceedings of the {Third} {Workshop} on {Discourse} in {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Miculicich Werlen, Lesly and Popescu-Belis, Andrei},
	month = sep,
	year = {2017},
	note = {00000},
	pages = {17--25}
}

@article{popescu-belis_context_2019,
	title = {Context in {Neural} {Machine} {Translation}: {A} {Review} of {Models} and {Evaluations}},
	shorttitle = {Context in {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1901.09115},
	abstract = {This review paper discusses how context has been used in neural machine translation (NMT) in the past two years (2017-2018). Starting with a brief retrospect on the rapid evolution of NMT models, the paper then reviews studies that evaluate NMT output from various perspectives, with emphasis on those analyzing limitations of the translation of contextual phenomena. In a subsequent version, the paper will then present the main methods that were proposed to leverage context for improving translation quality, and distinguishes methods that aim to improve the translation of specific phenomena from those that consider a wider unstructured context.},
	urldate = {2020-01-20},
	journal = {arXiv:1901.09115 [cs]},
	author = {Popescu-Belis, Andrei},
	month = jan,
	year = {2019},
	note = {00007 
arXiv: 1901.09115},
	keywords = {Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/YGWF8BFE/1901.html:text/html}
}

@inproceedings{bawden_evaluating_2018,
	address = {New Orleans, Louisiana},
	title = {Evaluating {Discourse} {Phenomena} in {Neural} {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/N18-1118},
	doi = {10.18653/v1/N18-1118},
	abstract = {For machine translation to tackle discourse phenomena, models must have access to extra-sentential linguistic context. There has been recent interest in modelling context in neural machine translation (NMT), but models have been principally evaluated with standard automatic metrics, poorly adapted to evaluating discourse phenomena. In this article, we present hand-crafted, discourse test sets, designed to test the models' ability to exploit previous source and target sentences. We investigate the performance of recently proposed multi-encoder NMT models trained on subtitles for English to French. We also explore a novel way of exploiting context from the previous sentence. Despite gains using BLEU, multi-encoder models give limited improvement in the handling of discourse phenomena: 50\% accuracy on our coreference test set and 53.5\% for coherence/cohesion (compared to a non-contextual baseline of 50\%). A simple strategy of decoding the concatenation of the previous and current sentence leads to good performance, and our novel strategy of multi-encoding and decoding of two sentences leads to the best performance (72.5\% for coreference and 57\% for coherence/cohesion), highlighting the importance of target-side context.},
	urldate = {2020-02-12},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Bawden, Rachel and Sennrich, Rico and Birch, Alexandra and Haddow, Barry},
	month = jun,
	year = {2018},
	note = {00055},
	keywords = {read},
	pages = {1304--1313}
}

@inproceedings{muller_large-scale_2018,
	address = {Brussels, Belgium},
	title = {A {Large}-{Scale} {Test} {Set} for the {Evaluation} of {Context}-{Aware} {Pronoun} {Translation} in {Neural} {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/W18-6307},
	doi = {10.18653/v1/W18-6307},
	abstract = {The translation of pronouns presents a special challenge to machine translation to this day, since it often requires context outside the current sentence. Recent work on models that have access to information across sentence boundaries has seen only moderate improvements in terms of automatic evaluation metrics such as BLEU. However, metrics that quantify the overall translation quality are ill-equipped to measure gains from additional context. We argue that a different kind of evaluation is needed to assess how well models translate inter-sentential phenomena such as pronouns. This paper therefore presents a test suite of contrastive translations focused specifically on the translation of pronouns. Furthermore, we perform experiments with several context-aware models. We show that, while gains in BLEU are moderate for those systems, they outperform baselines by a large margin in terms of accuracy on our contrastive test set. Our experiments also show the effectiveness of parameter tying for multi-encoder architectures.},
	urldate = {2020-02-25},
	booktitle = {Proceedings of the {Third} {Conference} on {Machine} {Translation}: {Research} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {MÃ¼ller, Mathias and Rios, Annette and Voita, Elena and Sennrich, Rico},
	month = oct,
	year = {2018},
	note = {00013},
	keywords = {read},
	pages = {61--72}
}

@inproceedings{guillou_automatic_2018,
	address = {Brussels, Belgium},
	title = {Automatic {Reference}-{Based} {Evaluation} of {Pronoun} {Translation} {Misses} the {Point}},
	url = {https://www.aclweb.org/anthology/D18-1513},
	doi = {10.18653/v1/D18-1513},
	abstract = {We compare the performance of the APT and AutoPRF metrics for pronoun translation against a manually annotated dataset comprising human judgements as to the correctness of translations of the PROTEST test suite. Although there is some correlation with the human judgements, a range of issues limit the performance of the automated metrics. Instead, we recommend the use of semi-automatic metrics and test suites in place of fully automatic metrics.},
	urldate = {2020-02-26},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Guillou, Liane and Hardmeier, Christian},
	month = oct,
	year = {2018},
	note = {00008},
	keywords = {read},
	pages = {4797--4802}
}

@article{post_call_2018,
	title = {A {Call} for {Clarity} in {Reporting} {BLEU} {Scores}},
	url = {http://arxiv.org/abs/1804.08771},
	abstract = {The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to "the" BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SacreBLEU, to facilitate this.},
	urldate = {2020-02-26},
	journal = {arXiv:1804.08771 [cs]},
	author = {Post, Matt},
	month = sep,
	year = {2018},
	note = {00100 
arXiv: 1804.08771},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/QT8X659E/1804.html:text/html}
}

@inproceedings{laubli_has_2018,
	address = {Brussels, Belgium},
	title = {Has {Machine} {Translation} {Achieved} {Human} {Parity}? {A} {Case} for {Document}-level {Evaluation}},
	shorttitle = {Has {Machine} {Translation} {Achieved} {Human} {Parity}?},
	url = {https://www.aclweb.org/anthology/D18-1512},
	doi = {10.18653/v1/D18-1512},
	abstract = {Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT ChineseâEnglish news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.},
	urldate = {2020-03-23},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {LÃ¤ubli, Samuel and Sennrich, Rico and Volk, Martin},
	month = oct,
	year = {2018},
	note = {00035},
	pages = {4791--4796}
}

@inproceedings{gong_document-level_2015,
	address = {Lisbon, Portugal},
	title = {Document-{Level} {Machine} {Translation} {Evaluation} with {Gist} {Consistency} and {Text} {Cohesion}},
	url = {https://www.aclweb.org/anthology/W15-2504},
	doi = {10.18653/v1/W15-2504},
	urldate = {2020-03-23},
	booktitle = {Proceedings of the {Second} {Workshop} on {Discourse} in {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Gong, Zhengxian and Zhang, Min and Zhou, Guodong},
	month = sep,
	year = {2015},
	note = {00006},
	pages = {33--40}
}

@article{jwalapuram_evaluating_2019,
	title = {Evaluating {Pronominal} {Anaphora} in {Machine} {Translation}: {An} {Evaluation} {Measure} and a {Test} {Suite}},
	shorttitle = {Evaluating {Pronominal} {Anaphora} in {Machine} {Translation}},
	url = {http://arxiv.org/abs/1909.00131},
	abstract = {The ongoing neural revolution in machine translation has made it easier to model larger contexts beyond the sentence-level, which can potentially help resolve some discourse-level ambiguities such as pronominal anaphora, thus enabling better translations. Unfortunately, even when the resulting improvements are seen as substantial by humans, they remain virtually unnoticed by traditional automatic evaluation measures like BLEU, as only a few words end up being affected. Thus, specialized evaluation measures are needed. With this aim in mind, we contribute an extensive, targeted dataset that can be used as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for English. We further propose an evaluation measure to differentiate good and bad pronoun translations. We also conduct a user study to report correlations with human judgments.},
	urldate = {2020-03-23},
	journal = {arXiv:1909.00131 [cs]},
	author = {Jwalapuram, Prathyusha and Joty, Shafiq and Temnikova, Irina and Nakov, Preslav},
	month = aug,
	year = {2019},
	note = {00002 
arXiv: 1909.00131},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/FZ8ZF9HL/1909.html:text/html}
}

@inproceedings{wong_extending_2012,
	address = {Jeju Island, Korea},
	title = {Extending {Machine} {Translation} {Evaluation} {Metrics} with {Lexical} {Cohesion} to {Document} {Level}},
	url = {https://www.aclweb.org/anthology/D12-1097},
	urldate = {2020-03-25},
	booktitle = {Proceedings of the 2012 {Joint} {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Wong, Billy T. M. and Kit, Chunyu},
	month = jul,
	year = {2012},
	note = {00044},
	pages = {1060--1068}
}

@inproceedings{papineni_bleu_2002,
	address = {Philadelphia, Pennsylvania, USA},
	title = {Bleu: a {Method} for {Automatic} {Evaluation} of {Machine} {Translation}},
	shorttitle = {Bleu},
	url = {https://www.aclweb.org/anthology/P02-1040},
	doi = {10.3115/1073083.1073135},
	urldate = {2020-03-25},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	month = jul,
	year = {2002},
	note = {10862},
	pages = {311--318}
}

@misc{noauthor_citeseerx_nodate,
	title = {{CiteSeerX} â {A} study of translation edit rate with targeted human annotation},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.129.4369},
	urldate = {2020-03-25},
	note = {00000},
	file = {CiteSeerX â A study of translation edit rate with targeted human annotation:/home/lupol/Zotero/storage/J8359PHW/summary.html:text/html}
}