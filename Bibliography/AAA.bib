
@article{voita_context-aware_2019,
	title = {Context-{Aware} {Monolingual} {Repair} for {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1909.01383},
	abstract = {Modern sentence-level NMT systems often produce plausible translations of isolated sentences. However, when put in context, these translations may end up being inconsistent with each other. We propose a monolingual DocRepair model to correct inconsistencies between sentence-level translations. DocRepair performs automatic post-editing on a sequence of sentence-level translations, refining translations of sentences in context of each other. For training, the DocRepair model requires only monolingual document-level data in the target language. It is trained as a monolingual sequence-to-sequence model that maps inconsistent groups of sentences into consistent ones. The consistent groups come from the original training data; the inconsistent groups are obtained by sampling round-trip translations for each isolated sentence. We show that this approach successfully imitates inconsistencies we aim to fix: using contrastive evaluation, we show large improvements in the translation of several contextual phenomena in an English-Russian translation task, as well as improvements in the BLEU score. We also conduct a human evaluation and show a strong preference of the annotators to corrected translations over the baseline ones. Moreover, we analyze which discourse phenomena are hard to capture using monolingual data only.},
	urldate = {2020-01-13},
	journal = {arXiv:1909.01383 [cs]},
	author = {Voita, Elena and Sennrich, Rico and Titov, Ivan},
	month = oct,
	year = {2019},
	note = {00003 
arXiv: 1909.01383},
	keywords = {Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/43RKRASU/1909.html:text/html}
}

@article{kim_when_2019,
	title = {When and {Why} is {Document}-level {Context} {Useful} in {Neural} {Machine} {Translation}?},
	url = {http://arxiv.org/abs/1910.00294},
	abstract = {Document-level context has received lots of attention for compensating neural machine translation (NMT) of isolated sentences. However, recent advances in document-level NMT focus on sophisticated integration of the context, explaining its improvement with only a few selected examples or targeted test sets. We extensively quantify the causes of improvements by a document-level model in general test sets, clarifying the limit of the usefulness of document-level context in NMT. We show that most of the improvements are not interpretable as utilizing the context. We also show that a minimal encoding is sufficient for the context modeling and very long context is not helpful for NMT.},
	urldate = {2020-01-13},
	journal = {arXiv:1910.00294 [cs]},
	author = {Kim, Yunsu and Tran, Duc Thanh and Ney, Hermann},
	month = oct,
	year = {2019},
	note = {00001 
arXiv: 1910.00294},
	keywords = {Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/AXHMMGK3/1910.html:text/html}
}

@inproceedings{tan_hierarchical_2019,
	address = {Hong Kong, China},
	title = {Hierarchical {Modeling} of {Global} {Context} for {Document}-{Level} {Neural} {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/D19-1168},
	doi = {10.18653/v1/D19-1168},
	abstract = {Document-level machine translation (MT) remains challenging due to the difficulty in efficiently using document context for translation. In this paper, we propose a hierarchical model to learn the global context for document-level neural machine translation (NMT). This is done through a sentence encoder to capture intra-sentence dependencies and a document encoder to model document-level inter-sentence consistency and coherence. With this hierarchical architecture, we feedback the extracted global document context to each word in a top-down fashion to distinguish different translations of a word according to its specific surrounding context. In addition, since large-scale in-domain document-level parallel corpora are usually unavailable, we use a two-step training strategy to take advantage of a large-scale corpus with out-of-domain parallel sentence pairs and a small-scale corpus with in-domain parallel document pairs to achieve the domain adaptability. Experimental results on several benchmark corpora show that our proposed model can significantly improve document-level translation performance over several strong NMT baselines.},
	urldate = {2020-01-13},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Tan, Xin and Zhang, Longyin and Xiong, Deyi and Zhou, Guodong},
	month = nov,
	year = {2019},
	note = {00002},
	keywords = {read},
	pages = {1576--1585}
}

@article{yu_putting_2019,
	title = {Putting {Machine} {Translation} in {Context} with the {Noisy} {Channel} {Model}},
	url = {http://arxiv.org/abs/1910.00553},
	abstract = {We show that Bayes' rule provides a compelling mechanism for controlling unconditional document language models, using the long-standing challenge of effectively leveraging document context in machine translation. In our formulation, we estimate the probability of a candidate translation as the product of the unconditional probability of the candidate output document and the ``reverse translation probability'' of translating the candidate output back into the input source language document---the so-called ``noisy channel'' decomposition. A particular advantage of our model is that it requires only parallel sentences to train, rather than parallel documents, which are not always available. Using a new beam search reranking approximation to solve the decoding problem, we find that document language models outperform language models that assume independence between sentences, and that using either a document or sentence language model outperforms comparable models that directly estimate the translation probability. We obtain the best-published results on the NIST Chinese--English translation task, a standard task for evaluating document translation. Our model also outperforms the benchmark Transformer model by approximately 2.5 BLEU on the WMT19 Chinese--English translation task.},
	urldate = {2020-01-13},
	journal = {arXiv:1910.00553 [cs]},
	author = {Yu, Lei and Sartran, Laurent and Stokowiec, Wojciech and Ling, Wang and Kong, Lingpeng and Blunsom, Phil and Dyer, Chris},
	month = oct,
	year = {2019},
	note = {00000 
arXiv: 1910.00553},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/GQ49QYF4/1910.html:text/html}
}

@article{li_pretrained_2019,
	title = {Pretrained {Language} {Models} for {Document}-{Level} {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1911.03110},
	abstract = {Previous work on document-level NMT usually focuses on limited contexts because of degraded performance on larger contexts. In this paper, we investigate on using large contexts with three main contributions: (1) Different from previous work which pertrained models on large-scale sentence-level parallel corpora, we use pretrained language models, specifically BERT, which are trained on monolingual documents; (2) We propose context manipulation methods to control the influence of large contexts, which lead to comparable results on systems using small and large contexts; (3) We introduce a multi-task training for regularization to avoid models overfitting our training corpora, which further improves our systems together with a deeper encoder. Experiments are conducted on the widely used IWSLT data sets with three language pairs, i.e., Chinese--English, French--English and Spanish--English. Results show that our systems are significantly better than three previously reported document-level systems.},
	urldate = {2020-01-13},
	journal = {arXiv:1911.03110 [cs]},
	author = {Li, Liangyou and Jiang, Xin and Liu, Qun},
	month = nov,
	year = {2019},
	note = {00001 
arXiv: 1911.03110},
	keywords = {Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/J6XRZ8AN/1911.html:text/html}
}

@article{dobreva_document_2019,
	title = {Document {Sub}-structure in {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1912.06598},
	abstract = {Current approaches to machine translation (MT) either translate sentences in isolation, disregarding the context they appear in, or model context on the level of the full document, without a notion of any internal structure the document may have. In this work we consider the fact that documents are rarely homogeneous blocks of text, but rather consist of parts covering different topics. Some documents, e.g. biographies and encyclopedia entries have highly predictable, regular structures in which sections are characterised by different topics. We draw inspiration from Louis and Webber (2014) who use this information to improve MT and transfer their proposal into the framework of neural MT. We compare two different methods of including information about the topic of the section within which each sentence is found: one using side constraints and the other using a cache-based model. We create and release the data on which we run our experiments -- parallel corpora for three language pairs (Chinese-English, French-English, Bulgarian-English) from Wikipedia biographies, preserving the boundaries of sections within the articles.},
	urldate = {2020-01-13},
	journal = {arXiv:1912.06598 [cs]},
	author = {Dobreva, Radina and Zhou, Jie and Bawden, Rachel},
	month = dec,
	year = {2019},
	note = {00000 
arXiv: 1912.06598},
	keywords = {Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/86G89QL7/1912.html:text/html}
}

@article{maruf_survey_2019,
	title = {A {Survey} on {Document}-level {Machine} {Translation}: {Methods} and {Evaluation}},
	shorttitle = {A {Survey} on {Document}-level {Machine} {Translation}},
	url = {http://arxiv.org/abs/1912.08494},
	abstract = {Machine translation (MT) is an important task in natural language processing (NLP) as it automates the translation process and reduces the reliance on human translators. With the advent of neural networks, the translation quality surpasses that of the translations obtained using statistical techniques. Up until three years ago, all neural translation models translated sentences independently, without incorporating any extra-sentential information. The aim of this paper is to highlight the major works that have been undertaken in the space of document-level machine translation before and after the neural revolution so that researchers can recognise where we started from and which direction we are heading in. When talking about the literature in statistical machine translation (SMT), we focus on works which have tried to improve the translation of specific discourse phenomena, while in neural machine translation (NMT), we focus on works which use the wider context explicitly. In addition to this, we also cover the evaluation strategies that have been introduced to account for the improvements in this domain.},
	urldate = {2020-01-13},
	journal = {arXiv:1912.08494 [cs]},
	author = {Maruf, Sameen and Saleh, Fahimeh and Haffari, Gholamreza},
	month = dec,
	year = {2019},
	note = {00000 
arXiv: 1912.08494},
	keywords = {Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/5C9VD4HE/1912.html:text/html;Maruf et al. - 2019 - A Survey on Document-level Machine Translation Me.pdf:/home/lupol/Zotero/storage/DTVQM8DI/Maruf et al. - 2019 - A Survey on Document-level Machine Translation Me.pdf:application/pdf}
}

@inproceedings{martinez_garcia_context-aware_2019,
	address = {Hong Kong, China},
	title = {Context-{Aware} {Neural} {Machine} {Translation} {Decoding}},
	url = {https://www.aclweb.org/anthology/D19-6502},
	doi = {10.18653/v1/D19-6502},
	abstract = {This work presents a decoding architecture that fuses the information from a neural translation model and the context semantics enclosed in a semantic space language model based on word embeddings. The method extends the beam search decoding process and therefore can be applied to any neural machine translation framework. With this, we sidestep two drawbacks of current document-level systems: (i) we do not modify the training process so there is no increment in training time, and (ii) we do not require document-level an-notated data. We analyze the impact of the fusion system approach and its parameters on the final translation quality for English–Spanish. We obtain consistent and statistically significant improvements in terms of BLEU and METEOR and we observe how the fused systems are able to handle synonyms to propose more adequate translations as well as help the system to disambiguate among several translation candidates for a word.},
	urldate = {2020-01-13},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {Discourse} in {Machine} {Translation} ({DiscoMT} 2019)},
	publisher = {Association for Computational Linguistics},
	author = {Martínez Garcia, Eva and Creus, Carles and España-Bonet, Cristina},
	month = nov,
	year = {2019},
	note = {00000},
	keywords = {read},
	pages = {13--23}
}

@inproceedings{sugiyama_data_2019,
	address = {Hong Kong, China},
	title = {Data augmentation using back-translation for context-aware neural machine translation},
	url = {https://www.aclweb.org/anthology/D19-6504},
	doi = {10.18653/v1/D19-6504},
	abstract = {A single sentence does not always convey information that is enough to translate it into other languages. Some target languages need to add or specialize words that are omitted or ambiguous in the source languages (e.g, zero pronouns in translating Japanese to English or epicene pronouns in translating English to French). To translate such ambiguous sentences, we need contexts beyond a single sentence, and have so far explored context-aware neural machine translation (NMT). However, a large amount of parallel corpora is not easily available to train accurate context-aware NMT models. In this study, we first obtain large-scale pseudo parallel corpora by back-translating monolingual data, and then investigate its impact on the translation accuracy of context-aware NMT models. We evaluated context-aware NMT models trained with small parallel corpora and the large-scale pseudo parallel corpora on English-Japanese and English-French datasets to demonstrate the large impact of the data augmentation for context-aware NMT models.},
	urldate = {2020-01-13},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {Discourse} in {Machine} {Translation} ({DiscoMT} 2019)},
	publisher = {Association for Computational Linguistics},
	author = {Sugiyama, Amane and Yoshinaga, Naoki},
	month = nov,
	year = {2019},
	note = {00000},
	keywords = {read},
	pages = {35--44}
}

@inproceedings{ohtani_context-aware_2019,
	address = {Hong Kong, China},
	title = {Context-aware {Neural} {Machine} {Translation} with {Coreference} {Information}},
	url = {https://www.aclweb.org/anthology/D19-6505},
	doi = {10.18653/v1/D19-6505},
	abstract = {We present neural machine translation models for translating a sentence in a text by using a graph-based encoder which can consider coreference relations provided within the text explicitly. The graph-based encoder can dynamically encode the source text without attending to all tokens in the text. In experiments, our proposed models provide statistically significant improvement to the previous approach of at most 0.9 points in the BLEU score on the OpenSubtitle2018 English-to-Japanese data set. Experimental results also show that the graph-based encoder can handle a longer text well, compared with the previous approach.},
	urldate = {2020-01-13},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {Discourse} in {Machine} {Translation} ({DiscoMT} 2019)},
	publisher = {Association for Computational Linguistics},
	author = {Ohtani, Takumi and Kamigaito, Hidetaka and Nagata, Masaaki and Okumura, Manabu},
	month = nov,
	year = {2019},
	note = {00000},
	keywords = {read},
	pages = {45--50}
}

@inproceedings{scherrer_analysing_2019,
	address = {Hong Kong, China},
	title = {Analysing concatenation approaches to document-level {NMT} in two different domains},
	url = {https://www.aclweb.org/anthology/D19-6506},
	doi = {10.18653/v1/D19-6506},
	abstract = {In this paper, we investigate how different aspects of discourse context affect the performance of recent neural MT systems. We describe two popular datasets covering news and movie subtitles and we provide a thorough analysis of the distribution of various document-level features in their domains. Furthermore, we train a set of context-aware MT models on both datasets and propose a comparative evaluation scheme that contrasts coherent context with artificially scrambled documents and absent context, arguing that the impact of discourse-aware MT models will become visible in this way. Our results show that the models are indeed affected by the manipulation of the test data, providing a different view on document-level translation quality than absolute sentence-level scores.},
	urldate = {2020-01-13},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {Discourse} in {Machine} {Translation} ({DiscoMT} 2019)},
	publisher = {Association for Computational Linguistics},
	author = {Scherrer, Yves and Tiedemann, Jörg and Loáiciga, Sharid},
	month = nov,
	year = {2019},
	note = {00000},
	keywords = {read},
	pages = {51--61}
}

@inproceedings{voita_when_2019,
	address = {Florence, Italy},
	title = {When a {Good} {Translation} is {Wrong} in {Context}: {Context}-{Aware} {Machine} {Translation} {Improves} on {Deixis}, {Ellipsis}, and {Lexical} {Cohesion}},
	shorttitle = {When a {Good} {Translation} is {Wrong} in {Context}},
	url = {https://www.aclweb.org/anthology/P19-1116},
	doi = {10.18653/v1/P19-1116},
	abstract = {Though machine translation errors caused by the lack of context beyond one sentence have long been acknowledged, the development of context-aware NMT systems is hampered by several problems. Firstly, standard metrics are not sensitive to improvements in consistency in document-level translations. Secondly, previous work on context-aware NMT assumed that the sentence-aligned parallel data consisted of complete documents while in most practical scenarios such document-level data constitutes only a fraction of the available parallel data. To address the first issue, we perform a human study on an English-Russian subtitles dataset and identify deixis, ellipsis and lexical cohesion as three main sources of inconsistency. We then create test sets targeting these phenomena. To address the second shortcoming, we consider a set-up in which a much larger amount of sentence-level data is available compared to that aligned at the document level. We introduce a model that is suitable for this scenario and demonstrate major gains over a context-agnostic baseline on our new benchmarks without sacrificing performance as measured with BLEU.},
	urldate = {2020-01-13},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Voita, Elena and Sennrich, Rico and Titov, Ivan},
	month = jul,
	year = {2019},
	note = {00007},
	keywords = {read},
	pages = {1198--1212}
}

@inproceedings{voita_context-aware_2018,
	address = {Melbourne, Australia},
	title = {Context-{Aware} {Neural} {Machine} {Translation} {Learns} {Anaphora} {Resolution}},
	url = {https://www.aclweb.org/anthology/P18-1117},
	doi = {10.18653/v1/P18-1117},
	abstract = {Standard machine translation systems process sentences in isolation and hence ignore extra-sentential information, even though extended context can both prevent mistakes in ambiguous cases and improve translation coherence. We introduce a context-aware neural machine translation model designed in such way that the flow of information from the extended context to the translation model can be controlled and analyzed. We experiment with an English-Russian subtitles dataset, and observe that much of what is captured by our model deals with improving pronoun translation. We measure correspondences between induced attention distributions and coreference relations and observe that the model implicitly captures anaphora. It is consistent with gains for sentences where pronouns need to be gendered in translation. Beside improvements in anaphoric cases, the model also improves in overall BLEU, both over its context-agnostic version (+0.7) and over simple concatenation of the context and source sentences (+0.6).},
	urldate = {2020-01-13},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Voita, Elena and Serdyukov, Pavel and Sennrich, Rico and Titov, Ivan},
	month = jul,
	year = {2018},
	note = {00047},
	keywords = {read},
	pages = {1264--1274}
}

@article{miculicich_document-level_2018,
	title = {Document-{Level} {Neural} {Machine} {Translation} with {Hierarchical} {Attention} {Networks}},
	url = {http://arxiv.org/abs/1809.01576},
	abstract = {Neural Machine Translation (NMT) can be improved by including document-level contextual information. For this purpose, we propose a hierarchical attention model to capture the context in a structured and dynamic manner. The model is integrated in the original NMT architecture as another level of abstraction, conditioning on the NMT model's own previous hidden states. Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.},
	urldate = {2020-01-20},
	journal = {arXiv:1809.01576 [cs]},
	author = {Miculicich, Lesly and Ram, Dhananjay and Pappas, Nikolaos and Henderson, James},
	month = oct,
	year = {2018},
	note = {00024 
arXiv: 1809.01576},
	keywords = {Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/JLC9BK82/1809.html:text/html}
}

@article{maruf_selective_2019,
	title = {Selective {Attention} for {Context}-aware {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1903.08788},
	abstract = {Despite the progress made in sentence-level NMT, current systems still fall short at achieving fluent, good quality translation for a full document. Recent works in context-aware NMT consider only a few previous sentences as context and may not scale to entire documents. To this end, we propose a novel and scalable top-down approach to hierarchical attention for context-aware NMT which uses sparse attention to selectively focus on relevant sentences in the document context and then attends to key words in those sentences. We also propose single-level attention approaches based on sentence or word-level information in the context. The document-level context representation, produced from these attention modules, is integrated into the encoder or decoder of the Transformer model depending on whether we use monolingual or bilingual context. Our experiments and evaluation on English-German datasets in different document MT settings show that our selective attention approach not only significantly outperforms context-agnostic baselines but also surpasses context-aware baselines in most cases.},
	urldate = {2020-01-20},
	journal = {arXiv:1903.08788 [cs]},
	author = {Maruf, Sameen and Martins, André F. T. and Haffari, Gholamreza},
	month = may,
	year = {2019},
	note = {00012},
	keywords = {Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/JG76XJZW/1903.html:text/html}
}

@article{popescu-belis_context_2019,
	title = {Context in {Neural} {Machine} {Translation}: {A} {Review} of {Models} and {Evaluations}},
	shorttitle = {Context in {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1901.09115},
	abstract = {This review paper discusses how context has been used in neural machine translation (NMT) in the past two years (2017-2018). Starting with a brief retrospect on the rapid evolution of NMT models, the paper then reviews studies that evaluate NMT output from various perspectives, with emphasis on those analyzing limitations of the translation of contextual phenomena. In a subsequent version, the paper will then present the main methods that were proposed to leverage context for improving translation quality, and distinguishes methods that aim to improve the translation of specific phenomena from those that consider a wider unstructured context.},
	urldate = {2020-01-20},
	journal = {arXiv:1901.09115 [cs]},
	author = {Popescu-Belis, Andrei},
	month = jan,
	year = {2019},
	note = {00007 
arXiv: 1901.09115},
	keywords = {Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/YGWF8BFE/1901.html:text/html}
}

@inproceedings{bawden_evaluating_2018,
	address = {New Orleans, Louisiana},
	title = {Evaluating {Discourse} {Phenomena} in {Neural} {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/N18-1118},
	doi = {10.18653/v1/N18-1118},
	abstract = {For machine translation to tackle discourse phenomena, models must have access to extra-sentential linguistic context. There has been recent interest in modelling context in neural machine translation (NMT), but models have been principally evaluated with standard automatic metrics, poorly adapted to evaluating discourse phenomena. In this article, we present hand-crafted, discourse test sets, designed to test the models' ability to exploit previous source and target sentences. We investigate the performance of recently proposed multi-encoder NMT models trained on subtitles for English to French. We also explore a novel way of exploiting context from the previous sentence. Despite gains using BLEU, multi-encoder models give limited improvement in the handling of discourse phenomena: 50\% accuracy on our coreference test set and 53.5\% for coherence/cohesion (compared to a non-contextual baseline of 50\%). A simple strategy of decoding the concatenation of the previous and current sentence leads to good performance, and our novel strategy of multi-encoding and decoding of two sentences leads to the best performance (72.5\% for coreference and 57\% for coherence/cohesion), highlighting the importance of target-side context.},
	urldate = {2020-02-12},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Bawden, Rachel and Sennrich, Rico and Birch, Alexandra and Haddow, Barry},
	month = jun,
	year = {2018},
	note = {00055},
	keywords = {read},
	pages = {1304--1313}
}

@article{jean_does_2017,
	title = {Does {Neural} {Machine} {Translation} {Benefit} from {Larger} {Context}?},
	url = {http://arxiv.org/abs/1704.05135},
	abstract = {We propose a neural machine translation architecture that models the surrounding text in addition to the source sentence. These models lead to better performance, both in terms of general translation quality and pronoun prediction, when trained on small corpora, although this improvement largely disappears when trained with a larger corpus. We also discover that attention-based neural machine translation is well suited for pronoun prediction and compares favorably with other approaches that were specifically designed for this task.},
	urldate = {2020-02-12},
	journal = {arXiv:1704.05135 [cs, stat]},
	author = {Jean, Sebastien and Lauly, Stanislas and Firat, Orhan and Cho, Kyunghyun},
	month = apr,
	year = {2017},
	note = {00038 
arXiv: 1704.05135},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/DGYHUQGJ/1704.html:text/html}
}

@inproceedings{wang_exploiting_2017,
	address = {Copenhagen, Denmark},
	title = {Exploiting {Cross}-{Sentence} {Context} for {Neural} {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/D17-1301},
	doi = {10.18653/v1/D17-1301},
	abstract = {In translation, considering the document as a whole can help to resolve ambiguities and inconsistencies. In this paper, we propose a cross-sentence context-aware approach and investigate the influence of historical contextual information on the performance of neural machine translation (NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points.},
	urldate = {2020-02-12},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Longyue and Tu, Zhaopeng and Way, Andy and Liu, Qun},
	month = sep,
	year = {2017},
	note = {00045},
	keywords = {read},
	pages = {2826--2831}
}

@inproceedings{tiedemann_neural_2017,
	address = {Copenhagen, Denmark},
	title = {Neural {Machine} {Translation} with {Extended} {Context}},
	url = {https://www.aclweb.org/anthology/W17-4811},
	doi = {10.18653/v1/W17-4811},
	abstract = {We investigate the use of extended context in attention-based neural machine translation. We base our experiments on translated movie subtitles and discuss the effect of increasing the segments beyond single translation units. We study the use of extended source language context as well as bilingual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases.},
	urldate = {2020-02-12},
	booktitle = {Proceedings of the {Third} {Workshop} on {Discourse} in {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Tiedemann, Jörg and Scherrer, Yves},
	month = sep,
	year = {2017},
	note = {00038},
	keywords = {read},
	pages = {82--92}
}

@article{tu_learning_2017,
	title = {Learning to {Remember} {Translation} {History} with a {Continuous} {Cache}},
	url = {http://arxiv.org/abs/1711.09367},
	abstract = {Existing neural machine translation (NMT) models generally translate sentences in isolation, missing the opportunity to take advantage of document-level information. In this work, we propose to augment NMT models with a very light-weight cache-like memory network, which stores recent hidden representations as translation history. The probability distribution over generated words is updated online depending on the translation history retrieved from the memory, endowing NMT models with the capability to dynamically adapt over time. Experiments on multiple domains with different topics and styles show the effectiveness of the proposed approach with negligible impact on the computational cost.},
	urldate = {2020-02-19},
	journal = {arXiv:1711.09367 [cs]},
	author = {Tu, Zhaopeng and Liu, Yang and Shi, Shuming and Zhang, Tong},
	month = nov,
	year = {2017},
	note = {00037 
arXiv: 1711.09367},
	keywords = {Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/NS4NTXHL/1711.html:text/html}
}

@inproceedings{kuang_modeling_2018,
	address = {Santa Fe, New Mexico, USA},
	title = {Modeling {Coherence} for {Neural} {Machine} {Translation} with {Dynamic} and {Topic} {Caches}},
	url = {https://www.aclweb.org/anthology/C18-1050},
	abstract = {Sentences in a well-formed text are connected to each other via various links to form the cohesive structure of the text. Current neural machine translation (NMT) systems translate a text in a conventional sentence-by-sentence fashion, ignoring such cross-sentence links and dependencies. This may lead to generate an incoherent target text for a coherent source text. In order to handle this issue, we propose a cache-based approach to modeling coherence for neural machine translation by capturing contextual information either from recently translated sentences or the entire document. Particularly, we explore two types of caches: a dynamic cache, which stores words from the best translation hypotheses of preceding sentences, and a topic cache, which maintains a set of target-side topical words that are semantically related to the document to be translated. On this basis, we build a new layer to score target words in these two caches with a cache-based neural model. Here the estimated probabilities from the cache-based neural model are combined with NMT probabilities into the final word prediction probabilities via a gating mechanism. Finally, the proposed cache-based neural model is trained jointly with NMT system in an end-to-end manner. Experiments and analysis presented in this paper demonstrate that the proposed cache-based model achieves substantial improvements over several state-of-the-art SMT and NMT baselines.},
	urldate = {2020-02-20},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Kuang, Shaohui and Xiong, Deyi and Luo, Weihua and Zhou, Guodong},
	month = aug,
	year = {2018},
	note = {00012},
	keywords = {read},
	pages = {596--606}
}

@inproceedings{muller_large-scale_2018,
	address = {Brussels, Belgium},
	title = {A {Large}-{Scale} {Test} {Set} for the {Evaluation} of {Context}-{Aware} {Pronoun} {Translation} in {Neural} {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/W18-6307},
	doi = {10.18653/v1/W18-6307},
	abstract = {The translation of pronouns presents a special challenge to machine translation to this day, since it often requires context outside the current sentence. Recent work on models that have access to information across sentence boundaries has seen only moderate improvements in terms of automatic evaluation metrics such as BLEU. However, metrics that quantify the overall translation quality are ill-equipped to measure gains from additional context. We argue that a different kind of evaluation is needed to assess how well models translate inter-sentential phenomena such as pronouns. This paper therefore presents a test suite of contrastive translations focused specifically on the translation of pronouns. Furthermore, we perform experiments with several context-aware models. We show that, while gains in BLEU are moderate for those systems, they outperform baselines by a large margin in terms of accuracy on our contrastive test set. Our experiments also show the effectiveness of parameter tying for multi-encoder architectures.},
	urldate = {2020-02-25},
	booktitle = {Proceedings of the {Third} {Conference} on {Machine} {Translation}: {Research} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Müller, Mathias and Rios, Annette and Voita, Elena and Sennrich, Rico},
	month = oct,
	year = {2018},
	note = {00013},
	keywords = {read},
	pages = {61--72}
}

@article{zheng_toward_2020,
	title = {Toward {Making} the {Most} of {Context} in {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/2002.07982},
	abstract = {Document-level machine translation manages to outperform sentence level models by a small margin, but have failed to be widely adopted. We argue that previous research did not make a clear use of the global context, and propose a new document-level NMT framework that deliberately models the local context of each sentence with the awareness of the global context of the document in both source and target languages. We specifically design the model to be able to deal with documents containing any number of sentences, including single sentences. This unified approach allows our model to be trained elegantly on standard datasets without needing to train on sentence and document level data separately. Experimental results demonstrate that our model outperforms Transformer baselines and previous document-level NMT models with substantial margins of up to 2.1 BLEU on state-of-the-art baselines. We also provide analyses which show the benefit of context far beyond the neighboring two or three sentences, which previous studies have typically incorporated.},
	urldate = {2020-02-26},
	journal = {arXiv:2002.07982 [cs]},
	author = {Zheng, Zaixiang and Yue, Xiang and Huang, Shujian and Chen, Jiajun and Birch, Alexandra},
	month = feb,
	year = {2020},
	note = {00000 
arXiv: 2002.07982},
	keywords = {Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/RKFHUT89/2002.html:text/html}
}

@inproceedings{agrawal_contextual_2018,
	title = {Contextual {Handling} in {Neural} {Machine} {Translation}: {Look} {Behind}, {Ahead} and on {Both} {Sides}},
	isbn = {978-84-09-01901-4},
	shorttitle = {Contextual {Handling} in {Neural} {Machine} {Translation}},
	url = {https://cris.fbk.eu/handle/11582/314425#.XlZ3xeF7mkA},
	abstract = {open},
	language = {eng},
	urldate = {2020-02-26},
	author = {Agrawal, Ruchit Rajeshkumar and Turchi, Marco and Negri, Matteo},
	year = {2018},
	note = {00007 
Accepted: 2018-08-08T15:15:28Z},
	keywords = {read},
	pages = {11--20},
	file = {Snapshot:/home/lupol/Zotero/storage/7UILJH35/314425.html:text/html}
}

@inproceedings{maruf_document_2018,
	address = {Melbourne, Australia},
	title = {Document {Context} {Neural} {Machine} {Translation} with {Memory} {Networks}},
	url = {https://www.aclweb.org/anthology/P18-1118},
	doi = {10.18653/v1/P18-1118},
	abstract = {We present a document-level neural machine translation model which takes both source and target document context into account using memory networks. We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables, i.e., the source sentences and their unobserved target translations in the document. The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components, one each for the source and target side, to capture the documental interdependencies. We train the model end-to-end, and propose an iterative decoding algorithm based on block coordinate descent. Experimental results of English translations from French, German, and Estonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.},
	urldate = {2020-02-26},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Maruf, Sameen and Haffari, Gholamreza},
	month = jul,
	year = {2018},
	note = {00032},
	keywords = {read},
	pages = {1275--1284}
}

@inproceedings{zhang_improving_2018,
	address = {Brussels, Belgium},
	title = {Improving the {Transformer} {Translation} {Model} with {Document}-{Level} {Context}},
	url = {https://www.aclweb.org/anthology/D18-1049},
	doi = {10.18653/v1/D18-1049},
	abstract = {Although the Transformer translation model (Vaswani et al., 2017) has achieved state-of-the-art performance in a variety of translation tasks, how to use document-level context to deal with discourse phenomena problematic for Transformer still remains a challenge. In this work, we extend the Transformer model with a new context encoder to represent document-level context, which is then incorporated into the original encoder and decoder. As large-scale document-level parallel corpora are usually not available, we introduce a two-step training method to take full advantage of abundant sentence-level parallel corpora and limited document-level parallel corpora. Experiments on the NIST Chinese-English datasets and the IWSLT French-English datasets show that our approach improves over Transformer significantly.},
	urldate = {2020-02-27},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Jiacheng and Luan, Huanbo and Sun, Maosong and Zhai, Feifei and Xu, Jingfang and Zhang, Min and Liu, Yang},
	month = oct,
	year = {2018},
	note = {00028},
	keywords = {read},
	pages = {533--542}
}

@article{jean_fill_2019,
	title = {Fill in the {Blanks}: {Imputing} {Missing} {Sentences} for {Larger}-{Context} {Neural} {Machine} {Translation}},
	shorttitle = {Fill in the {Blanks}},
	url = {http://arxiv.org/abs/1910.14075},
	abstract = {Most neural machine translation systems still translate sentences in isolation. To make further progress, a promising line of research additionally considers the surrounding context in order to provide the model potentially missing source-side information, as well as to maintain a coherent output. One difficulty in training such larger-context (i.e. document-level) machine translation systems is that context may be missing from many parallel examples. To circumvent this issue, two-stage approaches, in which sentence-level translations are post-edited in context, have recently been proposed. In this paper, we instead consider the viability of filling in the missing context. In particular, we consider three distinct approaches to generate the missing context: using random contexts, applying a copy heuristic or generating it with a language model. In particular, the copy heuristic significantly helps with lexical coherence, while using completely random contexts hurts performance on many long-distance linguistic phenomena. We also validate the usefulness of tagged back-translation. In addition to improving BLEU scores as expected, using back-translated data helps larger-context machine translation systems to better capture long-range phenomena.},
	urldate = {2020-02-27},
	journal = {arXiv:1910.14075 [cs]},
	author = {Jean, Sébastien and Bapna, Ankur and Firat, Orhan},
	month = oct,
	year = {2019},
	note = {00000 
arXiv: 1910.14075},
	keywords = {Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/UX4FWLQC/1910.html:text/html}
}

@inproceedings{fu_reference_2019,
	address = {Florence, Italy},
	title = {Reference {Network} for {Neural} {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/P19-1287},
	doi = {10.18653/v1/P19-1287},
	abstract = {Neural Machine Translation (NMT) has achieved notable success in recent years. Such a framework usually generates translations in isolation. In contrast, human translators often refer to reference data, either rephrasing the intricate sentence fragments with common terms in source language, or just accessing to the golden translation directly. In this paper, we propose a Reference Network to incorporate referring process into translation decoding of NMT. To construct a reference book, an intuitive way is to store the detailed translation history with extra memory, which is computationally expensive. Instead, we employ Local Coordinates Coding (LCC) to obtain global context vectors containing monolingual and bilingual contextual information for NMT decoding. Experimental results on Chinese-English and English-German tasks demonstrate that our proposed model is effective in improving the translation quality with lightweight computation cost.},
	urldate = {2020-02-27},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Fu, Han and Liu, Chenghao and Sun, Jianling},
	month = jul,
	year = {2019},
	note = {00000},
	keywords = {read},
	pages = {3002--3012}
}

@inproceedings{stojanovski_coreference_2018,
	address = {Brussels, Belgium},
	title = {Coreference and {Coherence} in {Neural} {Machine} {Translation}: {A} {Study} {Using} {Oracle} {Experiments}},
	shorttitle = {Coreference and {Coherence} in {Neural} {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/W18-6306},
	doi = {10.18653/v1/W18-6306},
	abstract = {Cross-sentence context can provide valuable information in Machine Translation and is critical for translation of anaphoric pronouns and for providing consistent translations. In this paper, we devise simple oracle experiments targeting coreference and coherence. Oracles are an easy way to evaluate the effect of different discourse-level phenomena in NMT using BLEU and eliminate the necessity to manually define challenge sets for this purpose. We propose two context-aware NMT models and compare them against models working on a concatenation of consecutive sentences. Concatenation models perform better, but are computationally expensive. We show that NMT models taking advantage of context oracle signals can achieve considerable gains in BLEU, of up to 7.02 BLEU for coreference and 1.89 BLEU for coherence on subtitles translation. Access to strong signals allows us to make clear comparisons between context-aware models.},
	urldate = {2020-03-04},
	booktitle = {Proceedings of the {Third} {Conference} on {Machine} {Translation}: {Research} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Stojanovski, Dario and Fraser, Alexander},
	month = oct,
	year = {2018},
	note = {00003},
	pages = {49--60}
}

@inproceedings{sukhbaatar_adaptive_2019,
	address = {Florence, Italy},
	title = {Adaptive {Attention} {Span} in {Transformers}},
	url = {https://www.aclweb.org/anthology/P19-1032},
	doi = {10.18653/v1/P19-1032},
	abstract = {We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.},
	urldate = {2020-03-04},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
	month = jul,
	year = {2019},
	note = {00012},
	pages = {331--335}
}

@inproceedings{cao_encoding_2018,
	address = {Brussels, Belgium},
	title = {Encoding {Gated} {Translation} {Memory} into {Neural} {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/D18-1340},
	doi = {10.18653/v1/D18-1340},
	abstract = {Translation memories (TM) facilitate human translators to reuse existing repetitive translation fragments. In this paper, we propose a novel method to combine the strengths of both TM and neural machine translation (NMT) for high-quality translation. We treat the target translation of a TM match as an additional reference input and encode it into NMT with an extra encoder. A gating mechanism is further used to balance the impact of the TM match on the NMT decoder. Experiment results on the UN corpus demonstrate that when fuzzy matches are higher than 50\%, the quality of NMT translation can be significantly improved by over 10 BLEU points.},
	urldate = {2020-03-04},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Cao, Qian and Xiong, Deyi},
	month = oct,
	year = {2018},
	note = {00005},
	pages = {3042--3047}
}

@article{jean_context-aware_2019,
	title = {Context-{Aware} {Learning} for {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1903.04715},
	abstract = {Interest in larger-context neural machine translation, including document-level and multi-modal translation, has been growing. Multiple works have proposed new network architectures or evaluation schemes, but potentially helpful context is still sometimes ignored by larger-context translation models. In this paper, we propose a novel learning algorithm that explicitly encourages a neural translation model to take into account additional context using a multilevel pair-wise ranking loss. We evaluate the proposed learning algorithm with a transformer-based larger-context translation system on document-level translation. By comparing performance using actual and random contexts, we show that a model trained with the proposed algorithm is more sensitive to the additional context.},
	urldate = {2020-03-10},
	journal = {arXiv:1903.04715 [cs]},
	author = {Jean, Sébastien and Cho, Kyunghyun},
	month = mar,
	year = {2019},
	note = {00003 
arXiv: 1903.04715},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/JB8PXLP7/1903.html:text/html}
}

@inproceedings{junczys-dowmunt_microsoft_2019,
	address = {Florence, Italy},
	title = {Microsoft {Translator} at {WMT} 2019: {Towards} {Large}-{Scale} {Document}-{Level} {Neural} {Machine} {Translation}},
	shorttitle = {Microsoft {Translator} at {WMT} 2019},
	url = {https://www.aclweb.org/anthology/W19-5321},
	doi = {10.18653/v1/W19-5321},
	abstract = {This paper describes the Microsoft Translator submissions to the WMT19 news translation shared task for English-German. Our main focus is document-level neural machine translation with deep transformer models. We start with strong sentence-level baselines, trained on large-scale data created via data-filtering and noisy back-translation and find that back-translation seems to mainly help with translationese input. We explore fine-tuning techniques, deeper models and different ensembling strategies to counter these effects. Using document boundaries present in the authentic and synthetic parallel data, we create sequences of up to 1000 subword segments and train transformer translation models. We experiment with data augmentation techniques for the smaller authentic data with document-boundaries and for larger authentic data without boundaries. We further explore multi-task training for the incorporation of document-level source language monolingual data via the BERT-objective on the encoder and two-pass decoding for combinations of sentence-level and document-level systems. Based on preliminary human evaluation results, evaluators strongly prefer the document-level systems over our comparable sentence-level system. The document-level systems also seem to score higher than the human references in source-based direct assessment.},
	urldate = {2020-03-11},
	booktitle = {Proceedings of the {Fourth} {Conference} on {Machine} {Translation} ({Volume} 2: {Shared} {Task} {Papers}, {Day} 1)},
	publisher = {Association for Computational Linguistics},
	author = {Junczys-Dowmunt, Marcin},
	month = aug,
	year = {2019},
	note = {00000},
	pages = {225--233}
}

@inproceedings{barrault_findings_2019,
	address = {Florence, Italy},
	title = {Findings of the 2019 {Conference} on {Machine} {Translation} ({WMT19})},
	url = {https://www.aclweb.org/anthology/W19-5301},
	doi = {10.18653/v1/W19-5301},
	abstract = {This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2019. Participants were asked to build machine translation systems for any of 18 language pairs, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. The task was also opened up to additional test suites to probe specific aspects of translation.},
	urldate = {2020-03-11},
	booktitle = {Proceedings of the {Fourth} {Conference} on {Machine} {Translation} ({Volume} 2: {Shared} {Task} {Papers}, {Day} 1)},
	publisher = {Association for Computational Linguistics},
	author = {Barrault, Loïc and Bojar, Ondřej and Costa-jussà, Marta R. and Federmann, Christian and Fishel, Mark and Graham, Yvette and Haddow, Barry and Huck, Matthias and Koehn, Philipp and Malmasi, Shervin and Monz, Christof and Müller, Mathias and Pal, Santanu and Post, Matt and Zampieri, Marcos},
	month = aug,
	year = {2019},
	note = {00000},
	pages = {1--61}
}

@inproceedings{joty_discotk_2014,
	address = {Baltimore, Maryland, USA},
	title = {{DiscoTK}: {Using} {Discourse} {Structure} for {Machine} {Translation} {Evaluation}},
	shorttitle = {{DiscoTK}},
	url = {https://www.aclweb.org/anthology/W14-3352},
	doi = {10.3115/v1/W14-3352},
	urldate = {2020-03-12},
	booktitle = {Proceedings of the {Ninth} {Workshop} on {Statistical} {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Joty, Shafiq and Guzmán, Francisco and Màrquez, Lluís and Nakov, Preslav},
	month = jun,
	year = {2014},
	note = {00000},
	pages = {402--408}
}

@article{laubli_has_2018,
	title = {Has {Machine} {Translation} {Achieved} {Human} {Parity}? {A} {Case} for {Document}-level {Evaluation}},
	shorttitle = {Has {Machine} {Translation} {Achieved} {Human} {Parity}?},
	url = {http://arxiv.org/abs/1808.07048},
	abstract = {Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.},
	urldate = {2020-03-13},
	journal = {arXiv:1808.07048 [cs]},
	author = {Läubli, Samuel and Sennrich, Rico and Volk, Martin},
	month = aug,
	year = {2018},
	note = {00035 
arXiv: 1808.07048},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/XPVL5NVB/1808.html:text/html}
}

@article{yang_enhancing_2019,
	title = {Enhancing {Context} {Modeling} with a {Query}-{Guided} {Capsule} {Network} for {Document}-level {Translation}},
	url = {http://arxiv.org/abs/1909.00564},
	doi = {10.18653/v1/D19-1164},
	abstract = {Context modeling is essential to generate coherent and consistent translation for Document-level Neural Machine Translations. The widely used method for document-level translation usually compresses the context information into a representation via hierarchical attention networks. However, this method neither considers the relationship between context words nor distinguishes the roles of context words. To address this problem, we propose a query-guided capsule networks to cluster context information into different perspectives from which the target translation may concern. Experiment results show that our method can significantly outperform strong baselines on multiple data sets of different domains.},
	urldate = {2020-03-13},
	journal = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	author = {Yang, Zhengxin and Zhang, Jinchao and Meng, Fandong and Gu, Shuhao and Feng, Yang and Zhou, Jie},
	year = {2019},
	note = {00001 
arXiv: 1909.00564},
	keywords = {Computer Science - Computation and Language},
	pages = {1527--1537},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/K4JFZESC/1909.html:text/html}
}

@article{xiong_modeling_2018,
	title = {Modeling {Coherence} for {Discourse} {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1811.05683},
	abstract = {Discourse coherence plays an important role in the translation of one text. However, the previous reported models most focus on improving performance over individual sentence while ignoring cross-sentence links and dependencies, which affects the coherence of the text. In this paper, we propose to use discourse context and reward to refine the translation quality from the discourse perspective. In particular, we generate the translation of individual sentences at first. Next, we deliberate the preliminary produced translations, and train the model to learn the policy that produces discourse coherent text by a reward teacher. Practical results on multiple discourse test datasets indicate that our model significantly improves the translation quality over the state-of-the-art baseline system by +1.23 BLEU score. Moreover, our model generates more discourse coherent text and obtains +2.2 BLEU improvements when evaluated by discourse metrics.},
	urldate = {2020-03-17},
	journal = {arXiv:1811.05683 [cs]},
	author = {Xiong, Hao and He, Zhongjun and Wu, Hua and Wang, Haifeng},
	month = nov,
	year = {2018},
	note = {00012},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/TQ2VC4Z7/1811.html:text/html}
}

@article{mace_using_2019,
	title = {Using {Whole} {Document} {Context} in {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1910.07481},
	abstract = {In Machine Translation, considering the document as a whole can help to resolve ambiguities and inconsistencies. In this paper, we propose a simple yet promising approach to add contextual information in Neural Machine Translation. We present a method to add source context that capture the whole document with accurate boundaries, taking every word into account. We provide this additional information to a Transformer model and study the impact of our method on three language pairs. The proposed approach obtains promising results in the English-German, English-French and French-English document-level translation tasks. We observe interesting cross-sentential behaviors where the model learns to use document-level information to improve translation coherence.},
	urldate = {2020-03-18},
	journal = {arXiv:1910.07481 [cs]},
	author = {Macé, Valentin and Servan, Christophe},
	month = oct,
	year = {2019},
	note = {00000 
arXiv: 1910.07481},
	keywords = {Computer Science - Computation and Language, read},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/9MNAB9ML/1910.html:text/html}
}

@article{jwalapuram_evaluating_2019,
	title = {Evaluating {Pronominal} {Anaphora} in {Machine} {Translation}: {An} {Evaluation} {Measure} and a {Test} {Suite}},
	shorttitle = {Evaluating {Pronominal} {Anaphora} in {Machine} {Translation}},
	url = {http://arxiv.org/abs/1909.00131},
	abstract = {The ongoing neural revolution in machine translation has made it easier to model larger contexts beyond the sentence-level, which can potentially help resolve some discourse-level ambiguities such as pronominal anaphora, thus enabling better translations. Unfortunately, even when the resulting improvements are seen as substantial by humans, they remain virtually unnoticed by traditional automatic evaluation measures like BLEU, as only a few words end up being affected. Thus, specialized evaluation measures are needed. With this aim in mind, we contribute an extensive, targeted dataset that can be used as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for English. We further propose an evaluation measure to differentiate good and bad pronoun translations. We also conduct a user study to report correlations with human judgments.},
	urldate = {2020-03-23},
	journal = {arXiv:1909.00131 [cs]},
	author = {Jwalapuram, Prathyusha and Joty, Shafiq and Temnikova, Irina and Nakov, Preslav},
	month = aug,
	year = {2019},
	note = {00002 
arXiv: 1909.00131},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/lupol/Zotero/storage/D5RPJII9/1909.html:text/html}
}